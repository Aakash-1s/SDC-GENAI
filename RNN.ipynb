{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOstk4Es0kjp1zcl27b4MN2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aakash-1s/SDC-GENAI/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRueqLtAAMSx",
        "outputId": "f2a22339-dd57-4f68-b426-31b83850f0bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 33.00860\n",
            "Epoch 1000, Loss: 2.28901\n",
            "Epoch 2000, Loss: 2.28948\n",
            "Epoch 3000, Loss: 2.28989\n",
            "Epoch 4000, Loss: 2.29024\n",
            "Epoch 5000, Loss: 2.29054\n",
            "Epoch 6000, Loss: 2.29077\n",
            "Epoch 7000, Loss: 2.29096\n",
            "Epoch 8000, Loss: 2.29110\n",
            "Epoch 9000, Loss: 2.29122\n",
            "\n",
            "Final Predictions:\n",
            "Input: 0, Predicted Next Value: 1.9960\n",
            "Input: 1, Predicted Next Value: 4.5771\n",
            "Input: 2, Predicted Next Value: 4.5771\n",
            "Input: 3, Predicted Next Value: 4.5771\n",
            "Input: 4, Predicted Next Value: 4.5771\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the activation function (tanh for hidden state)\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Derivative of tanh (for backpropagation)\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.square(np.tanh(x))\n",
        "\n",
        "# Initialize the dataset (simple sequence for prediction)\n",
        "# Let's use a sequence of numbers, and the goal is to predict the next number in the sequence\n",
        "X = np.array([[0], [1], [2], [3], [4], [5]])  # Example sequence\n",
        "y = np.array([[1], [2], [3], [4], [5], [6]])  # Target sequence (the next value)\n",
        "\n",
        "# Initialize the RNN parameters\n",
        "input_size = 1  # Input size (each time step has one feature)\n",
        "hidden_size = 5  # Number of hidden neurons\n",
        "output_size = 1  # Output size (we're predicting one value at a time)\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)  # For reproducibility\n",
        "W_xh = np.random.randn(input_size, hidden_size)  # Weights from input to hidden\n",
        "W_hh = np.random.randn(hidden_size, hidden_size)  # Weights from hidden to hidden (recurrent)\n",
        "W_hy = np.random.randn(hidden_size, output_size)  # Weights from hidden to output\n",
        "b_h = np.zeros((1, hidden_size))  # Bias for hidden layer\n",
        "b_y = np.zeros((1, output_size))  # Bias for output layer\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "# Training the RNN\n",
        "for epoch in range(epochs):\n",
        "    hidden_state = np.zeros((1, hidden_size))  # Initialize hidden state to zero\n",
        "\n",
        "    # Forward propagation\n",
        "    for t in range(len(X) - 1):\n",
        "        # Get the current input and the target output\n",
        "        x_t = X[t].reshape(1, -1)\n",
        "        y_t = y[t + 1].reshape(1, -1)\n",
        "\n",
        "        # Compute the hidden state at time step t\n",
        "        hidden_state = tanh(np.dot(x_t, W_xh) + np.dot(hidden_state, W_hh) + b_h)\n",
        "\n",
        "        # Compute the output at time step t\n",
        "        output = np.dot(hidden_state, W_hy) + b_y\n",
        "\n",
        "        # Calculate the loss (Mean Squared Error)\n",
        "        loss = np.mean(np.square(y_t - output))\n",
        "\n",
        "        # Backpropagation through time (BPTT)\n",
        "        output_error = y_t - output\n",
        "        output_delta = output_error\n",
        "\n",
        "        # Backpropagate the error through the hidden-to-output weights\n",
        "        hidden_error = output_delta.dot(W_hy.T) * tanh_derivative(hidden_state)\n",
        "\n",
        "        # Update weights and biases using gradient descent\n",
        "        W_hy += hidden_state.T.dot(output_delta) * learning_rate\n",
        "        b_y += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        W_xh += x_t.T.dot(hidden_error) * learning_rate\n",
        "        W_hh += hidden_state.T.dot(hidden_error) * learning_rate\n",
        "        b_h += np.sum(hidden_error, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.5f}\")\n",
        "\n",
        "# After training, let's print the final predictions\n",
        "print(\"\\nFinal Predictions:\")\n",
        "hidden_state = np.zeros((1, hidden_size))  # Reset the hidden state\n",
        "for t in range(len(X) - 1):\n",
        "    x_t = X[t].reshape(1, -1)\n",
        "    hidden_state = tanh(np.dot(x_t, W_xh) + np.dot(hidden_state, W_hh) + b_h)\n",
        "    output = np.dot(hidden_state, W_hy) + b_y\n",
        "    print(f\"Input: {X[t][0]}, Predicted Next Value: {output[0][0]:.4f}\")\n"
      ]
    }
  ]
}